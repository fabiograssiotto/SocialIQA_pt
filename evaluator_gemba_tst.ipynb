{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## This file implements translation evaluations using a modified GEMBA prompt system for the test dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Install Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install pandas\n",
    "%pip install langchain\n",
    "%pip install langchain-openai"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Langchain\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_openai import ChatOpenAI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Globals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LLM_MODEL_NAME=\"gpt-3.5-turbo\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Utility function to set up the OpenAI API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_openai_key():\n",
    "    try:\n",
    "        # Open and read the entire content of the file\n",
    "        with open(\"openai-key.txt\", 'r') as file:\n",
    "            contents = file.read()\n",
    "        \n",
    "        return contents\n",
    "    \n",
    "    except FileNotFoundError:\n",
    "        print(f\"The file does not exist.\")\n",
    "        return None\n",
    "    except Exception as e:\n",
    "        # Handle other potential exceptions (e.g., permission errors)\n",
    "        print(f\"An error occurred while reading the file: {str(e)}\")\n",
    "        return None\n",
    "\n",
    "openai_key = load_openai_key()\n",
    "os.environ[\"OPENAI_API_KEY\"] = openai_key\n",
    "\n",
    "chat = ChatOpenAI(model=\"gpt-3.5-turbo\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prompt definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "system = \"\"\"\\\n",
    "You are a helpful evaluator of the quality of translations.\n",
    "Score the following translations from English to Portuguese on a continuous scale from 0 to 100, \n",
    "where a score of zero means \"no meaning preserved\" and score of one hundred means\n",
    "\"perfect meaning and grammar\".\n",
    "\"\"\"\n",
    "\n",
    "human = \"\"\"\\\n",
    "English source: {source_seg}\n",
    "Portuguese translation 1: \"{target_seg1}\" \n",
    "Portuguese translation 2: \"{target_seg2}\"\n",
    "Portuguese translation 3: \"{target_seg3}\"\n",
    "Reply with only the scores of your evaluation:\n",
    "Score 1:\n",
    "Score 2:\n",
    "Score 3:\n",
    "\"\"\"\n",
    "prompt = ChatPromptTemplate.from_messages([(\"system\", system), (\"human\", human)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initial test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chain = prompt | chat\n",
    "response = chain.invoke(input={\"source_seg\": \"There are more things between heaven and earth\",\n",
    "                    \"target_seg1\": \"Existem mais coisas entre o céu e a terra\",\n",
    "                    \"target_seg2\": \"Existem coisas entre o céu e a terra\",\n",
    "                    \"target_seg3\": \"Existem coisas entre o paraíso e a terra\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(response.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Translation Datasets for comparisons (TEST)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tst = pd.read_csv('data/tst.csv')\n",
    "df_tst = df_tst.drop(columns=['Unnamed: 0'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tst1_pt = pd.read_csv('translated/marian_mt/tst_pt.csv')\n",
    "df_tst2_pt = pd.read_csv('translated/unicamp_t5/tst_pt.csv')\n",
    "df_tst3_pt = pd.read_csv('translated/nllb_1.3b/tst_pt.csv')\n",
    "\n",
    "# clean up data\n",
    "df_tst1_pt = df_tst1_pt.drop(columns=['Unnamed: 0.1', 'Unnamed: 0'])\n",
    "df_tst2_pt = df_tst2_pt.drop(columns=['Unnamed: 0.1', 'Unnamed: 0', 'correct'])\n",
    "df_tst3_pt = df_tst3_pt.drop(columns=['Unnamed: 0.1', 'Unnamed: 0', 'correct'])\n",
    "\n",
    "# Rename columns\n",
    "df_tst1_pt = df_tst1_pt.rename(columns={'context': 'context_1', 'question': 'question_1',\n",
    "                                  'answerA': 'answerA_1', 'answerB': 'answerB_1',\n",
    "                                  'answerC': 'answerC_1'})\n",
    "df_tst2_pt = df_tst2_pt.rename(columns={'context': 'context_2', 'question': 'question_2',\n",
    "                                  'answerA': 'answerA_2', 'answerB': 'answerB_2',\n",
    "                                  'answerC': 'answerC_2'})\n",
    "df_tst3_pt = df_tst3_pt.rename(columns={'context': 'context_3', 'question': 'question_3',\n",
    "                                  'answerA': 'answerA_3', 'answerB': 'answerB_3',\n",
    "                                  'answerC': 'answerC_3'})\n",
    "# Merge the three dataframes\n",
    "df_tst_pt = pd.concat((df_tst1_pt, df_tst2_pt, df_tst3_pt), axis=1)\n",
    "df_tst_pt = df_tst_pt[['context_1', 'context_2', 'context_3',\n",
    "                 'question_1', 'question_2', 'question_3',\n",
    "                 'answerA_1', 'answerA_2', 'answerA_3',\n",
    "                 'answerB_1', 'answerB_2', 'answerB_3',\n",
    "                 'answerC_1', 'answerC_2', 'answerC_3',\n",
    "                 'correct']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tst_pt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_best_translations(df_src_lang, df_tgt_lang):\n",
    "\n",
    "    df_output = df_src_lang.copy()\n",
    "    df_output.loc[:, ['context', 'question', 'answerA', 'answerB', 'answerC']] = None\n",
    "\n",
    "    for i in tqdm(range(len(df_src_lang))):\n",
    "\n",
    "        sentence_list = ['context', 'question', 'answerA', 'answerB', 'answerC']\n",
    "        chain = prompt | chat\n",
    "\n",
    "        for sentence in sentence_list:\n",
    "\n",
    "            # Candidate translations\n",
    "            sentence_t1 = sentence + \"_1\"\n",
    "            sentence_t2 = sentence + \"_2\"\n",
    "            sentence_t3 = sentence + \"_3\"\n",
    "\n",
    "            response = chain.invoke(input={\"source_seg\": df_src_lang.loc[i, sentence],\n",
    "                            \"target_seg1\": df_tgt_lang.loc[i, sentence_t1],\n",
    "                            \"target_seg2\": df_tgt_lang.loc[i, sentence_t2],\n",
    "                            \"target_seg3\": df_tgt_lang.loc[i, sentence_t3]})\n",
    "            results = [int(line.split(': ')[1]) for line in response.content.strip().split('\\n')]\n",
    "            \n",
    "            best = results.index(max(results))\n",
    "            col_best = sentence + '_' + str(best+1)\n",
    "            best_result = df_tgt_lang.loc[i, col_best]\n",
    "            sentence_idx = sentence_list.index(sentence)\n",
    "            df_output.iat[i, sentence_idx] = best_result\n",
    "\n",
    "    return df_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dev base\n",
    "df_tst_final = get_best_translations(df_tst, df_tst_pt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tst_final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tst_final.to_csv('translated/final/tst.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml_proj",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
